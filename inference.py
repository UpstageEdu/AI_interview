# inference.py
"""
4-bit ì–‘ìí™” ëª¨ë¸ ìš”ì•½ ì¶”ë¡ 
"""
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import   PeftModel
from utils.prompts import TEMPLATE

CONFIG = dict(
    ckpt = "checkpoints/gpt2-lora/checkpoint-100",
    question = "íŒ€ ë‚´ ê°ˆë“±ì„ í•´ê²°í•œ ê²½í—˜ì´ ìˆë‚˜ìš”?",
    answer   = "í”„ë¡œì íŠ¸ ì¼ì • ì§€ì—° ë•Œ ì˜ê²¬ ì¶©ëŒì´ ìˆì—ˆì§€ë§Œ ì›ë§Œí•˜ê²Œ í•´ê²°í•˜ê¸° ìœ„í•´, ëª¨ë‘ì—ê²Œ ì¹œì ˆí•˜ê³ , ìŠ¤ìŠ¤ë¡œ ì¼ë„ ì—´ì‹¬íˆ í–ˆë˜ ê²½í—˜ì´ ìˆìŠµë‹ˆë‹¤.",
)

def main(cfg = CONFIG):
    tok   = AutoTokenizer.from_pretrained("gpt2")
    model = AutoModelForCausalLM.from_pretrained(
        "gpt2",  device_map="auto")
    
    model=PeftModel.from_pretrained(model, cfg["ckpt"])
    
    model.eval()

    prompt = TEMPLATE.format(
        instruction="ë‹¤ìŒ ë©´ì ‘ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ì½ê³  í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”.",
        input=f"ì§ˆë¬¸: {cfg['question']}\në‹µë³€: {cfg['answer']}"
    )
    ids = tok(prompt, return_tensors="pt").to(model.device)
    out = model.generate(**ids, max_new_tokens=120, temperature=0.2, top_p=0.9)
    summary = tok.decode(out[0], skip_special_tokens=True)\
                 .split("### Response:\n")[-1].strip()
    print("\nğŸ“„ ìš”ì•½:\n", summary)

if __name__ == "__main__":
    main()
